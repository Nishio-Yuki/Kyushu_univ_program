{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1455408a-d78c-478d-9512-c141e1f42078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 08:39:34 | INFO | [読込] Arrow を読み込み中: /work/eval_dataset/Cop1KO_isp_mouse_tokenize_dataset/data-00000-of-00001.arrow\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/work/eval_dataset/Cop1KO_isp_mouse_tokenize_dataset/data-00000-of-00001.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 306\u001b[0m\n\u001b[1;32m    303\u001b[0m ensure_dirs(clean_tmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# 読み込み & 分割\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_arrow_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARROW_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ds_wt, ds_ko \u001b[38;5;241m=\u001b[39m split_dataset(ds)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# ベース2状態を保存（WT/KO）\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 181\u001b[0m, in \u001b[0;36mload_arrow_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_arrow_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    180\u001b[0m     log_step(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[読込] Arrow を読み込み中: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    182\u001b[0m     log_step(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[読込] 行数=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 列=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m CONDITION_COL \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcolumn_names, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONDITION_COL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 列が見つかりません\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/load.py:1397\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1393\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1394\u001b[0m )\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1397\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/load.py:1137\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1137\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/load.py:913\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    904\u001b[0m \n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/load.py:527\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    522\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    523\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[1;32m    526\u001b[0m )\n\u001b[0;32m--> 527\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m module_path, \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m _PACKAGED_DATASETS_MODULES[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m    535\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_files,\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    538\u001b[0m }\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/data_files.py:701\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    696\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    698\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m         patterns_for_key\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 701\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     )\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/data_files.py:594\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 594\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/datasets/data_files.py:383\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/work/eval_dataset/Cop1KO_isp_mouse_tokenize_dataset/data-00000-of-00001.arrow'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter版：複数遺伝子 同時削除オプション → UMAP（PDFのみ・列は不変）\n",
    "# - 既存の Arrow（列: `input_ids`, `cell_types`, `organ_major`, `disease`, `length`）をそのまま使用\n",
    "# - `disease` は Cop1_WT / Cop1_KO を前提\n",
    "# - KO行に対し、指定した複数遺伝子の token を**同時に削除**（列は不変、`length`のみ再計算）\n",
    "# - WT, KO, KO(各Delシナリオ...) を**同一UMAP空間**へ投影、PDF保存\n",
    "# - EmbExtractor: MODEL_TYPE=\"Pretrained\", num_classes=0（int）, `model_directory`はextract_embs()で渡す\n",
    "\n",
    "# %%\n",
    "# === Cell 1: 依存関係の確認 / 必要ならインストール ===\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def _ensure(pkg, pip_name=None):\n",
    "    pip_name = pip_name or pkg\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", pip_name])\n",
    "        importlib.import_module(pkg)\n",
    "\n",
    "# tqdmのIProgress問題を回避（ipywidgetsが無い環境向け）\n",
    "try:\n",
    "    import ipywidgets  # noqa\n",
    "except Exception:\n",
    "    from tqdm import auto as tqdmauto\n",
    "    import tqdm.notebook as tqdmb\n",
    "    tqdmb.tqdm = tqdmauto.tqdm\n",
    "    tqdmb.trange = tqdmauto.trange\n",
    "    tqdmb.tnrange = tqdmauto.trange\n",
    "\n",
    "for pkg, pipn in [\n",
    "    (\"datasets\", \"datasets\"),\n",
    "    (\"umap\", \"umap-learn\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "]:\n",
    "    _ensure(pkg, pipn)\n",
    "\n",
    "# Geneformer はリポジトリから利用想定（環境依存のためpipインストールは行わない）\n",
    "\n",
    "# %%\n",
    "# === Cell 2: インポート & ロギング設定 ===\n",
    "import os, pickle, json, shutil, time, math, random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Iterable, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "import umap\n",
    "import logging\n",
    "\n",
    "# Geneformer EmbExtractor\n",
    "from geneformer.emb_extractor import EmbExtractor\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "log = logging.getLogger(\"UMAP-MultiDel\")\n",
    "\n",
    "def log_step(msg: str):\n",
    "    log.info(msg)\n",
    "\n",
    "# %%\n",
    "# === Cell 3: パラメータ ===\n",
    "# ----- 入力 Arrow -----\n",
    "ARROW_PATH = \"/work/eval_dataset/Cop1KO_isp_mouse_tokenize_dataset/data-00000-of-00001.arrow\"  # ←あなたのArrowパス\n",
    "# /work/scRNA-seq_data/Adrenal_scRNA-seq_MS/output_arrow/data-00000-of-00001.arrow\n",
    "# /work/eval_dataset/Cop1KO_isp_mouse_tokenize_dataset/data-00000-of-00001.arrow\n",
    "CONDITION_COL = \"disease\"\n",
    "\n",
    "\n",
    "# ==== Cell 3: パラメータの追加/変更 ====\n",
    "# どちらを削除ベースにするか: \"WT\" or \"KO\"\n",
    "BASE_FOR_DELETION = \"KO\"  \n",
    "\n",
    "VALUE_KO = \"Cop1_KO\" # 遺伝子削除を行う方のdisease名を指定\n",
    "VALUE_WT = \"Cop1_WT\" # 遺伝子削除後に近づけたい目標状態を指定\n",
    "\n",
    "# シナリオ（削除する遺伝子集合）—例\n",
    "MULTI_DELETE_GENE_SETS = [\n",
    "    [\"Apoe\"], \n",
    "    # [\"Apoe\", \"Lrp1\"],\n",
    "    # [\"Apoe\", \"Lrp1\", \"Ldlr\"],\n",
    "]\n",
    "\n",
    "\n",
    "# ----- 辞書（提供済み） -----\n",
    "PATH_GENE_SYMBOL2ENS = \"/work/mouse-geneformer/dictionary_pickle/MLM-re_token_dictionary_v1_GeneSymbol_to_EnsemblID.pkl\"\n",
    "TOKEN_DICTIONARY_FILE = \"/work/mouse-geneformer/dictionary_pickle/MLM-re_token_dictionary_v1.pkl\"\n",
    "\n",
    "\n",
    "# ----- 作業/出力 -----\n",
    "WORK_DIR = Path(\"/work/kyushu_univ/Umap\")\n",
    "OUT_DIR  = WORK_DIR / \"out\"\n",
    "EMB_CSV_DIR = OUT_DIR / \"emb_csv\"\n",
    "OUT_PREFIX = \"UMAP_Cpo1KO\"  # 出力PDFファイル名の接頭辞\n",
    "\n",
    "# ----- 一時 .dataset 出力（シナリオごとに動的作成） -----\n",
    "TMP_WT = WORK_DIR / \"tmp_WT.dataset\"\n",
    "TMP_KO = WORK_DIR / \"tmp_KO.dataset\"\n",
    "TMP_SCENARIO_ROOT = WORK_DIR / \"tmp_scenarios\"  # 中に各シナリオの .dataset を作成\n",
    "\n",
    "# ----- Geneformer（EmbExtractor）設定 -----\n",
    "FINETUNED_MODEL_DIR = \"/work/kyushu_univ/cop1_pretrain\"  # ←あなたのモデルパス\n",
    "# /work/results/251021_mouse-geneformer_CellClassifier_nan_L2048_B12_LR5e-05_LSlinear_WU500_E20_OadamW_F0_ISP-nan\n",
    "# /work/kyushu_univ/cop1_pretrain\n",
    "MODEL_TYPE = \"Pretrained\"   # {\"Pretrained\",\"GeneClassifier\",\"CellClassifier\"}\n",
    "EMB_LAYER = 0\n",
    "FORWARD_BATCH_SIZE = 10\n",
    "NPROC = 8\n",
    "MAX_NCELLS_PER_STATE = 5000  # UMAP負荷対策\n",
    "\n",
    "# ----- UMAP 設定 -----\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST    = 0.1\n",
    "UMAP_METRIC      = \"cosine\"\n",
    "UMAP_RANDOM_STATE= 42\n",
    "\n",
    "# ----- 描画設定 -----\n",
    "POINT_SIZE = 1.5\n",
    "ALPHA = 0.85\n",
    "SAVE_PDF = True\n",
    "\n",
    "np.random.seed(UMAP_RANDOM_STATE)\n",
    "random.seed(UMAP_RANDOM_STATE)\n",
    "\n",
    "# %%\n",
    "# === Cell 4: ユーティリティ ===\n",
    "\n",
    "# キャッシュして高速化\n",
    "_SYM2ENS_CACHE = None\n",
    "_ENS2TOK_CACHE = None\n",
    "def _load_dicts():\n",
    "    global _SYM2ENS_CACHE, _ENS2TOK_CACHE\n",
    "    if _SYM2ENS_CACHE is None:\n",
    "        with open(PATH_GENE_SYMBOL2ENS, \"rb\") as f:\n",
    "            _SYM2ENS_CACHE = pickle.load(f)\n",
    "    if _ENS2TOK_CACHE is None:\n",
    "        with open(TOKEN_DICTIONARY_FILE, \"rb\") as f:\n",
    "            _ENS2TOK_CACHE = pickle.load(f)\n",
    "\n",
    "def ensure_dirs(clean_tmp=True):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    EMB_CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    if clean_tmp:\n",
    "        for p in (TMP_WT, TMP_KO, TMP_SCENARIO_ROOT):\n",
    "            if p.exists():\n",
    "                shutil.rmtree(p)\n",
    "    TMP_SCENARIO_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def resolve_token_ids(symbols: Iterable[str]) -> Dict[str, int]:\n",
    "    \"\"\"GeneSymbol -> token_id の辞書を返す（全て解決できない場合はエラー）\"\"\"\n",
    "    _load_dicts()\n",
    "    sym2tok = {}\n",
    "    missing_syms = []\n",
    "    for sym in symbols:\n",
    "        ens = _SYM2ENS_CACHE.get(sym)\n",
    "        if ens is None:\n",
    "            missing_syms.append(sym)\n",
    "            continue\n",
    "        tok = _ENS2TOK_CACHE.get(ens)\n",
    "        if isinstance(tok, (int, np.integer)):\n",
    "            sym2tok[sym] = int(tok)\n",
    "        else:\n",
    "            missing_syms.append(sym)\n",
    "    if missing_syms:\n",
    "        raise ValueError(f\"辞書に見つからない/解決できない遺伝子がありました: {missing_syms}\")\n",
    "    # ログ\n",
    "    for s, t in sym2tok.items():\n",
    "        log_step(f\"[辞書] {s} → token_id={t}\")\n",
    "    return sym2tok\n",
    "\n",
    "def load_arrow_dataset(path: str) -> Dataset:\n",
    "    log_step(f\"[読込] Arrow を読み込み中: {path}\")\n",
    "    ds = load_dataset(\"arrow\", data_files=path)[\"train\"]\n",
    "    log_step(f\"[読込] 行数={len(ds)}, 列={ds.column_names}\")\n",
    "    assert CONDITION_COL in ds.column_names, f\"{CONDITION_COL} 列が見つかりません\"\n",
    "    return ds\n",
    "\n",
    "def split_dataset(ds: Dataset):\n",
    "    log_step(f\"[分割] disease == '{VALUE_WT}' / '{VALUE_KO}' でフィルタ\")\n",
    "    ds_wt = ds.filter(lambda x: x[CONDITION_COL] == VALUE_WT, desc=\"filter WT\")\n",
    "    ds_ko = ds.filter(lambda x: x[CONDITION_COL] == VALUE_KO, desc=\"filter KO\")\n",
    "    log_step(f\"[分割] WT={len(ds_wt)} 行, KO={len(ds_ko)} 行\")\n",
    "    assert len(ds_wt) > 0 and len(ds_ko) > 0, \"WT/KO が見つかりませんでした\"\n",
    "    return ds_wt, ds_ko\n",
    "\n",
    "def remove_tokens_from_ids(ids: List[int], token_ids: set) -> List[int]:\n",
    "    if not token_ids:\n",
    "        return list(ids)\n",
    "    return [t for t in ids if t not in token_ids]\n",
    "\n",
    "def make_del_dataset_in_memory(ds_ko: Dataset, del_token_ids: set) -> Dataset:\n",
    "    \"\"\"\n",
    "    KOの input_ids から del_token_ids を**同時に**削除し、length を再計算。\n",
    "    列スキーマは不変（新カラムは作らない）。\n",
    "    \"\"\"\n",
    "    gene_count = len(del_token_ids)\n",
    "    log_step(f\"[DelMap] KOに対し {gene_count} 遺伝子のtokenを同時削除 → 仮想KO_Del\")\n",
    "    def _map_fn(ex):\n",
    "        new_ids = remove_tokens_from_ids(ex[\"input_ids\"], del_token_ids)\n",
    "        ex[\"input_ids\"] = new_ids\n",
    "        if \"length\" in ex:\n",
    "            ex[\"length\"] = len(new_ids)\n",
    "        return ex\n",
    "    return ds_ko.map(_map_fn, desc=f\"remove {gene_count} tokens from KO\", load_from_cache_file=False)\n",
    "\n",
    "def save_to_disk(ds: Dataset, out_dir: Path):\n",
    "    log_step(f\"[保存] save_to_disk → {out_dir}\")\n",
    "    ds.save_to_disk(str(out_dir))\n",
    "\n",
    "def scenario_label_from_genes(genes: List[str], base_label: str) -> str:\n",
    "    \"\"\"凡例用ラベル: e.g., Cop1_WT_Del[Apoe+Lrp1] or Cop1_KO_Del[...]\"\"\"\n",
    "    genes_sorted = \"+\".join(sorted(genes))\n",
    "    return f\"{base_label}_Del[{genes_sorted}]\"\n",
    "\n",
    "def extract_embs_from_dir(dataset_dir: Path, state_tag: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    - dataset_dir: save_to_disk した .dataset のフォルダ\n",
    "    - state_tag:   出力CSVの接頭辞\n",
    "    戻り値: (n_cells, emb_dim)\n",
    "    \"\"\"\n",
    "    log_step(f\"[埋込] EmbExtractor 初期化\")\n",
    "    embex = EmbExtractor(\n",
    "        model_type=MODEL_TYPE,\n",
    "        num_classes=0,\n",
    "        emb_mode=\"cell\",\n",
    "        cell_emb_style=\"mean_pool\",\n",
    "        filter_data=None,\n",
    "        max_ncells=MAX_NCELLS_PER_STATE,\n",
    "        emb_layer=EMB_LAYER,\n",
    "        emb_label=None,\n",
    "        labels_to_plot=None,\n",
    "        forward_batch_size=FORWARD_BATCH_SIZE,\n",
    "        nproc=NPROC,\n",
    "        summary_stat=None,\n",
    "        token_dictionary_file=TOKEN_DICTIONARY_FILE,\n",
    "    )\n",
    "    EMB_CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    log_step(f\"[埋込] extract_embs(model_directory=..., input_data_file=..., output_directory=..., output_prefix={state_tag})\")\n",
    "    embs_df = embex.extract_embs(\n",
    "        model_directory=FINETUNED_MODEL_DIR,\n",
    "        input_data_file=str(dataset_dir),\n",
    "        output_directory=str(EMB_CSV_DIR),\n",
    "        output_prefix=state_tag,\n",
    "    )\n",
    "    embs = embs_df.to_numpy(dtype=float, copy=False)\n",
    "    log_step(f\"[埋込] shape={embs.shape}, saved CSV under {EMB_CSV_DIR}\")\n",
    "    return embs\n",
    "\n",
    "def run_umap(X: np.ndarray) -> np.ndarray:\n",
    "    log_step(\"[UMAP] fit_transform 開始 …\")\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        metric=UMAP_METRIC,\n",
    "        random_state=UMAP_RANDOM_STATE,\n",
    "        n_components=2,\n",
    "    )\n",
    "    coords = reducer.fit_transform(X)\n",
    "    log_step(\"[UMAP] 完了\")\n",
    "    return coords\n",
    "\n",
    "def _auto_colors(n: int) -> List[str]:\n",
    "    \"\"\"matplotlibのタブカラーパレットから必要数確保（n>10なら再利用）\"\"\"\n",
    "    base = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])\n",
    "    if not base:\n",
    "        base = [f\"C{i}\" for i in range(10)]\n",
    "    if n <= len(base):\n",
    "        return base[:n]\n",
    "    out = []\n",
    "    i = 0\n",
    "    for k in range(n):\n",
    "        out.append(base[i % len(base)])\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def plot_pdf(coords: np.ndarray, labels: np.ndarray, legend_order: List[str], colors_map: Dict[str, str],\n",
    "             title: str, out_pdf: Path):\n",
    "    log_step(f\"[描画] PDF 保存: {out_pdf}\")\n",
    "    plt.figure(figsize=(7.8, 6.8))\n",
    "    for k in legend_order:\n",
    "        pts = coords[labels == k]\n",
    "        if len(pts) == 0:\n",
    "            continue\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], s=POINT_SIZE, alpha=ALPHA, label=k, c=colors_map.get(k, None))\n",
    "    plt.legend(title=\"state\", markerscale=3, loc=\"best\", frameon=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()\n",
    "\n",
    "# %%\n",
    "# === Cell 5: 実行フロー ===\n",
    "start_ts = time.time()\n",
    "ensure_dirs(clean_tmp=True)\n",
    "\n",
    "# 読み込み & 分割\n",
    "ds = load_arrow_dataset(ARROW_PATH)\n",
    "ds_wt, ds_ko = split_dataset(ds)\n",
    "\n",
    "# ベース2状態を保存（WT/KO）\n",
    "save_to_disk(ds_wt, TMP_WT)\n",
    "save_to_disk(ds_ko, TMP_KO)\n",
    "\n",
    "# ★ ここから追加：削除ベースの選択\n",
    "if BASE_FOR_DELETION.upper() == \"WT\":\n",
    "    ds_base = ds_wt\n",
    "    base_label = VALUE_WT\n",
    "else:\n",
    "    ds_base = ds_ko\n",
    "    base_label = VALUE_KO\n",
    "\n",
    "# 削除シナリオの token_id 解析\n",
    "all_symbols = sorted({s for subset in MULTI_DELETE_GENE_SETS for s in subset})\n",
    "sym2tok = resolve_token_ids(all_symbols) if all_symbols else {}\n",
    "\n",
    "# シナリオごとに .dataset 作成\n",
    "scenario_dirs: List[Tuple[str, Path]] = []  # (label, dir)\n",
    "scenario_dirs.append((VALUE_WT, TMP_WT))\n",
    "scenario_dirs.append((VALUE_KO, TMP_KO))\n",
    "\n",
    "for genes in MULTI_DELETE_GENE_SETS:\n",
    "    label = scenario_label_from_genes(genes, base_label=base_label)\n",
    "    out_dir = TMP_SCENARIO_ROOT / f\"{label}.dataset\"\n",
    "    if out_dir.exists():\n",
    "        shutil.rmtree(out_dir)\n",
    "    token_ids = {sym2tok[g] for g in genes} if genes else set()\n",
    "    ds_del = make_del_dataset_in_memory(ds_base, token_ids)  # ★ WT もしくは KO に対して削除\n",
    "    save_to_disk(ds_del, out_dir)\n",
    "    scenario_dirs.append((label, out_dir))\n",
    "\n",
    "# 埋め込み抽出（順序＝凡例順に利用）\n",
    "emb_arrays = []\n",
    "labels_vec = []\n",
    "legend_order = [lbl for (lbl, _) in scenario_dirs]\n",
    "\n",
    "for label, ddir in scenario_dirs:\n",
    "    arr = extract_embs_from_dir(ddir, state_tag=label)\n",
    "    emb_arrays.append(arr)\n",
    "    labels_vec.extend([label]*len(arr))\n",
    "\n",
    "# UMAP\n",
    "X_all = np.vstack(emb_arrays)\n",
    "y = np.array(labels_vec)\n",
    "coords = run_umap(X_all)\n",
    "\n",
    "# 色\n",
    "palette = _auto_colors(len(legend_order))\n",
    "color_map = {lbl: palette[i] for i, lbl in enumerate(legend_order)}\n",
    "\n",
    "# PDF 保存\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_pdf = OUT_DIR / f\"{OUT_PREFIX}.pdf\"\n",
    "plot_pdf(\n",
    "    coords, y, legend_order, color_map,\n",
    "    title=f\"UMAP: {VALUE_WT} vs {VALUE_KO} vs Multi-Gene Deletions (KO base)\",\n",
    "    out_pdf=out_pdf\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_ts\n",
    "log_step(f\"[完了] 出力: {out_pdf}\")\n",
    "log_step(f\"[完了] 所要時間: {elapsed/60:.2f} 分\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341674c-a9f6-4524-83b8-e93b378108e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
